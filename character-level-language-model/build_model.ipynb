{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_for_training='shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 94275 total characters and 38 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open(input_text_for_training, 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: ':', 10: ';', 11: '?', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbasedTextGenerator:\n",
    "    \n",
    "    def __init__(self, input_text_for_training, learning_rate ):\n",
    "        self.bar = 1\n",
    "        self.input_text_for_training = input_text_for_training\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "            \n",
    "    def read_text_file(self):\n",
    "        input_text_for_training=self.input_text_for_training\n",
    "        data = open(input_text_for_training, 'r').read()\n",
    "        data= data.lower()\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        #print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))  \n",
    "        char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "        ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "        return chars, vocab_size, char_to_ix, ix_to_char, data\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def smooth(self,loss, cur_loss):\n",
    "        return loss * 0.999 + cur_loss * 0.001\n",
    "    \n",
    "    def print_sample(self,sample_ix):\n",
    "        ix_to_char=self.read_text_file()[3]\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "        print ('%s' % (txt, ), end='')\n",
    "    \n",
    "    def get_initial_loss(self,seq_length):\n",
    "        vocab_size=self.read_text_file()[1]\n",
    "        return -np.log(1.0/vocab_size)*seq_length\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def initialize_parameters(self,n_a, n_x, n_y):\n",
    "        \"\"\"\n",
    "        Initialize parameters with small random values\n",
    "        \n",
    "        Returns:\n",
    "        parameters -- python dictionary containing:\n",
    "                            Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                            Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                            Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                            b --  Bias, numpy array of shape (n_a, 1)\n",
    "                            by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "        Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "        Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "        b = np.zeros((n_a, 1)) # hidden bias\n",
    "        by = np.zeros((n_y, 1)) # output bias\n",
    "        \n",
    "        parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def rnn_step_forward(self,parameters, a_prev, x):\n",
    "        \n",
    "        Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "        a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "        p_t = self.softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "        \n",
    "        return a_next, p_t\n",
    "    \n",
    "    def rnn_step_backward(self,dy, gradients, parameters, x, a, a_prev):\n",
    "        \n",
    "        gradients['dWya'] += np.dot(dy, a.T)\n",
    "        gradients['dby'] += dy\n",
    "        da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "        daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "        gradients['db'] += daraw\n",
    "        gradients['dWax'] += np.dot(daraw, x.T)\n",
    "        gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "        gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self,parameters, gradients, lr):\n",
    "    \n",
    "        parameters['Wax'] += -lr * gradients['dWax']\n",
    "        parameters['Waa'] += -lr * gradients['dWaa']\n",
    "        parameters['Wya'] += -lr * gradients['dWya']\n",
    "        parameters['b']  += -lr * gradients['db']\n",
    "        parameters['by']  += -lr * gradients['dby']\n",
    "        return parameters\n",
    "    \n",
    "    def rnn_forward(self,X, Y, a0, parameters):\n",
    "        vocab_size=self.read_text_file()[1]\n",
    "        # Initialize x, a and y_hat as empty dictionaries\n",
    "        x, a, y_hat = {}, {}, {}\n",
    "        \n",
    "        a[-1] = np.copy(a0)\n",
    "        \n",
    "        # initialize your loss to 0\n",
    "        loss = 0\n",
    "        \n",
    "        for t in range(len(X)):\n",
    "            \n",
    "            # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "            # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "            x[t] = np.zeros((vocab_size,1)) \n",
    "            if (X[t] != None):\n",
    "                x[t][X[t]] = 1\n",
    "            \n",
    "            # Run one step forward of the RNN\n",
    "            a[t], y_hat[t] = self.rnn_step_forward(parameters, a[t-1], x[t])\n",
    "            \n",
    "            # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "            loss -= np.log(y_hat[t][Y[t],0])\n",
    "            \n",
    "        cache = (y_hat, a, x)\n",
    "            \n",
    "        return loss, cache\n",
    "    \n",
    "    def rnn_backward(self,X, Y, parameters, cache):\n",
    "        # Initialize gradients as an empty dictionary\n",
    "        gradients = {}\n",
    "        \n",
    "        # Retrieve from cache and parameters\n",
    "        (y_hat, a, x) = cache\n",
    "        Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "        \n",
    "        # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "        gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "        gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "        gradients['da_next'] = np.zeros_like(a[0])\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(len(X))):\n",
    "            dy = np.copy(y_hat[t])\n",
    "            dy[Y[t]] -= 1\n",
    "            gradients = self.rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return gradients, a\n",
    "    \n",
    "    def clip(self,gradients, maxValue):\n",
    "        '''\n",
    "        Clips the gradients' values between minimum and maximum.\n",
    "        \n",
    "        Arguments:\n",
    "        gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "        maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "        \n",
    "        Returns: \n",
    "        gradients -- a dictionary with the clipped gradients.\n",
    "        '''\n",
    "        \n",
    "        dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "       \n",
    "        # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby].\n",
    "        for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "             np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "        \n",
    "        gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def sample(self,parameters,  seed):\n",
    "        \"\"\"\n",
    "        Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "    \n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "        char_to_ix -- python dictionary mapping each character to an index.\n",
    "        seed -- used for grading purposes. Do not worry about it.\n",
    "    \n",
    "        Returns:\n",
    "        indices -- a list of length n containing the indices of the sampled characters.\n",
    "        \"\"\"\n",
    "        char_to_ix=self.read_text_file()[2]\n",
    "        # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "        Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "        vocab_size = by.shape[0]\n",
    "        n_a = Waa.shape[1]\n",
    "    \n",
    "        # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). \n",
    "        x = np.zeros([vocab_size,1])\n",
    "        # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "        a_prev = np.zeros([n_a,1])\n",
    "        \n",
    "        # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate \n",
    "        indices = []\n",
    "        \n",
    "        # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "        idx = -1 \n",
    "        \n",
    "        # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "        # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "        # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "        counter = 0\n",
    "        newline_character = char_to_ix['\\n']\n",
    "        \n",
    "        while (idx != newline_character and counter != 50):\n",
    "            \n",
    "            # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "            a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n",
    "            z = np.dot(Wya,a)+by\n",
    "            y = self.softmax(z)\n",
    "            \n",
    "            # for grading purposes\n",
    "            np.random.seed(counter+seed) \n",
    "            \n",
    "            # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "            idx = np.random.choice(range(len(y)),p=y.ravel())\n",
    "    \n",
    "            # Append the index to \"indices\"\n",
    "            indices.append(idx)\n",
    "            \n",
    "            # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "            x = np.zeros((vocab_size,1))\n",
    "            x[idx] = 1\n",
    "            \n",
    "            # Update \"a_prev\" to be \"a\"\n",
    "            a_prev = a\n",
    "            \n",
    "            # for grading purposes\n",
    "            seed += 1\n",
    "            counter +=1\n",
    "            \n",
    "        if (counter == 50):\n",
    "            indices.append(char_to_ix['\\n'])\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def optimize(self,X, Y, a_prev, parameters):\n",
    "        \"\"\"\n",
    "        Execute one step of the optimization to train the model.\n",
    "        \n",
    "        Arguments:\n",
    "        X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "        Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "        a_prev -- previous hidden state.\n",
    "        parameters -- python dictionary containing:\n",
    "                            Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                            Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                            Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                            b --  Bias, numpy array of shape (n_a, 1)\n",
    "                            by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "        learning_rate -- learning rate for the model.\n",
    "        \n",
    "        Returns:\n",
    "        loss -- value of the loss function (cross-entropy)\n",
    "        gradients -- python dictionary containing:\n",
    "                            dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                            dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                            dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                            db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                            dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "        a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "        \"\"\"\n",
    "        learning_rate =self.learning_rate\n",
    "        # Forward propagate through time \n",
    "        loss, cache = self.rnn_forward(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # Backpropagate through time \n",
    "        gradients, a = self.rnn_backward(X, Y, parameters, cache)\n",
    "        \n",
    "        # Clip gradients between -5 (min) and 5 (max) \n",
    "        gradients = self.clip(gradients, 5)\n",
    "        \n",
    "        # Update parameters \n",
    "        parameters = self.update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        return loss, gradients, a[len(X)-1]\n",
    "    \n",
    "    def model(self, num_iterations, n_a, seq_length):\n",
    "        \"\"\"\n",
    "        Trains the model and generates text \n",
    "        \n",
    "        Arguments:\n",
    "        data -- text corpus\n",
    "        ix_to_char -- dictionary that maps the index to a character\n",
    "        char_to_ix -- dictionary that maps a character to an index\n",
    "        num_iterations -- number of iterations to train the model for\n",
    "        n_a -- number of units of the RNN cell\n",
    "        seq_length -- number of lines to sample at each iteration. \n",
    "        vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "        \n",
    "        Returns:\n",
    "        parameters -- learned parameters\n",
    "        \"\"\"\n",
    "        data=self.read_text_file()[4]\n",
    "        ix_to_char=self.read_text_file()[3]\n",
    "        char_to_ix=self.read_text_file()[2]\n",
    "        vocab_size=self.read_text_file()[1]\n",
    "        input_text_for_training=self.input_text_for_training\n",
    "        # Retrieve n_x and n_y from vocab_size\n",
    "        n_x, n_y = vocab_size, vocab_size\n",
    "        \n",
    "        # Initialize parameters\n",
    "        parameters = self.initialize_parameters(n_a, n_x, n_y)\n",
    "        \n",
    "        # Initialize loss (this is required because we want to smooth our loss)\n",
    "        loss = self.get_initial_loss(seq_length)\n",
    "        \n",
    "        # Build list of all lines (training examples).\n",
    "        with open(input_text_for_training) as f:\n",
    "            examples = f.readlines()\n",
    "        examples = [x.lower().strip() for x in examples]\n",
    "        \n",
    "        # Shuffle list of all lines\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(examples)\n",
    "        \n",
    "        # Initialize the hidden state of your LSTM\n",
    "        a_prev = np.zeros((n_a, 1))\n",
    "        \n",
    "        # Optimization loop\n",
    "        for j in range(num_iterations):\n",
    "    \n",
    "            index = j % len(examples)\n",
    "            X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "            Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "            \n",
    "            # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "            curr_loss, gradients, a_prev = self.optimize(X, Y, a_prev, parameters)\n",
    "            \n",
    "            \n",
    "            # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "            loss = self.smooth(loss, curr_loss)\n",
    "    \n",
    "            # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "            if j % 2000 == 0:\n",
    "                \n",
    "                print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "                \n",
    "                # The number of dinosaur names to print\n",
    "                seed = 0\n",
    "                for name in range(seq_length):\n",
    "                    \n",
    "                    # Sample indices and print them\n",
    "                    sampled_indices = self.sample(parameters, seed)\n",
    "                    self.print_sample(sampled_indices)\n",
    "                    \n",
    "                    seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "          \n",
    "                print('\\n')\n",
    "            \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=LSTMbasedTextGenerator('shakespeare.txt',0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 36.466804\n",
      "\n",
      "Ieyvvr)h.mk-y?pmu pcdctr\n",
      "Di.!\n",
      "Eyvvr)h.mk-y?pmu pcdctr\n",
      "I.!\n",
      "Yvvr)h.mk-y?pmu pcdctr\n",
      ".!\n",
      "Vvr)h.mk-y?pmu pcdctr\n",
      "!\n",
      "Vr)h.mk-y?pmu pcdctr\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 82.458852\n",
      "\n",
      "Metsrochare wello sheast lopy han is fian,\n",
      "In.\n",
      "Iutis mecmet,\n",
      "Me aens a lorpeeeth cy hit buth nearith his foisme\n",
      "Wous med hy row the to ireve,\n",
      "E aers a loroogete cy hit buth nearith his foisme,\n",
      "Tor meer wello sheast lopy han is fian,\n",
      "A dou banpmy het.\n",
      "To iand wello sheast lopy han is fian,\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 81.339331\n",
      "\n",
      "Nowrtigh ridt:,\n",
      "Ind aith batteralese by ing byen seasoume,\n",
      "Ithtr,\n",
      "Ne agnind ionn hith by hit byen seathth,\n",
      "Wous:,\n",
      "Eabers',\n",
      "Tood; of whiss qilfss lovy haofat-e,\n",
      "Abest batser,\n",
      "Theld,\n",
      " douk bltes-eive by ing byen seasoume,\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 80.203916\n",
      "\n",
      "Nowhtr,\n",
      "Inca doth bith,\n",
      "Ithtt thinet spraun:\n",
      "Ol akit.\n",
      "Wour nher werss simes,\n",
      "Be doul bith,\n",
      "Us do pray-ss,\n",
      "Abess bart, de wikdt fot croued butt,\n",
      "Till so whing the vianeds fald wear of do thee, ba\n",
      "-erse' or theay,\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 78.694831\n",
      "\n",
      "Intsen in pive.\n",
      "For all,\n",
      "Furtiend neweres poreng mumy eale the ble-forn,\n",
      "In all,\n",
      "Wnou imarcumins poollibes whe ffith hare colle sin\n",
      "Af eruld my theave' yalld arrfend onore rorch spoo\n",
      "Thou cor whiou on my and whe halr deare braus, lof\n",
      "Abest a full dive,\n",
      "Thee shave.\n",
      "-artee in theave' yalld arrfend onore rorch spoof\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 77.215332\n",
      "\n",
      "Introw of meseris selfes lorugh leat eod:\n",
      "Fle aglon,\n",
      "Guth,\n",
      "In adjue angaum'ss fay hese, so leblese.\n",
      "Wour me,\n",
      "Ad exse blign'inse cy foud word,\n",
      "To dedingresou on lovasley gald wake i afe's.\n",
      "Abe:\n",
      "Tide my ullln the.\n",
      "All-cace,\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 75.973132\n",
      "\n",
      "Nownis leos whot.\n",
      "In baise beaust fost by groccrored doftered if hou\n",
      "Ithis jemict por shilliann,,\n",
      "Nic from anots fothads for buted efent,\n",
      "Wost ill,\n",
      "Be coric, ver forop yem ulest do contile:\n",
      "Ut.\n",
      "Aakis beaust fost by groccrored doftered if hour,\n",
      "\n",
      "Us leot whot.\n",
      "Allf-canser entie,,\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 75.346615\n",
      "\n",
      "Nowisn shinds tom sheast\n",
      "In bairin besss heth'gvangr but heild whel so matn\n",
      "Ithrike so whink shangn\n",
      "Ne all maded uncith cuilld by.\n",
      "Worn shipding ware kor notul be gue mare evering o\n",
      "Be hoth being fothant live wordd dotore spoll worl\n",
      "Uth of thy sor the to my you hart face am cored th\n",
      "A dite anink fith due staity for sore her gomming \n",
      "Us me,\n",
      "Alle baruen mith dwall,\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 74.704262\n",
      "\n",
      "Intlrgare,\n",
      "Hich from aning four cullen asseare shoulce,\n",
      "Huth deatins thy shall,\n",
      "Igd cone ang the wiles groeds,\n",
      "Woth lell whosu, inatiang,:\n",
      "Ai erve ang the wiles groeds,\n",
      "Uth neme whosu, in'ss loute and weem lice,\n",
      "Aad, cains the wilet for asphand povel not offore \n",
      "Till nofure,\n",
      "And hade,\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 74.783381\n",
      "\n",
      "Physt compilt thy this telont i hapt gidn alpath t\n",
      "In e,\n",
      "Kruth of thy tis this telont i hapt gidn alpath th\n",
      "Pacains a swere oted wand bave ese,\n",
      "Wort of thy tis this telont i hapt gidn alpath tho\n",
      "Be doth angey hewelds infaeth hind'ot if thee thou\n",
      "Wored thes thy this telont i hapt gidn alpath thos\n",
      "A doth angey hewelds infaeth hind'ot if thee thou \n",
      "Wing thes thy this telont i hapt gidn alpath thos \n",
      "'houge or umbesje wefsleaton sedkers goth sengen g\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 75.015548\n",
      "\n",
      "Hewnlone on thougend!\n",
      "Be a doth bemitaker cay dite use,\n",
      "By thee pomteres peing and you hank be in deatiked\n",
      "Hea doth belith love weest as, biights feep.\n",
      "Worn meingh serase,\n",
      "Ag,\n",
      "To chene, but as mene hinsce geat?\n",
      "A doth bemhs cfore weest as.\n",
      "Thee rebughin seefas miny dave thee me clave plea \n",
      "And deart theen dey is bettled beauth reed?\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 73.478749\n",
      "\n",
      "Shy time pray thy shigh,\n",
      "In fair:\n",
      "Ktost of thy ssparined,\n",
      "Secairn)\n",
      "Wotlarchansiqse shink and you haru enat bears coup\n",
      "Ab so eed unt ind day my best,\n",
      "To chariey soull litiarr you haru enat bears coup,\n",
      "A doth bestine whe well,\n",
      "Their seserer\n",
      "'ise a pret gevealy deve tine eis, not their sim d\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 74.076675\n",
      "\n",
      "Onturong staunspraspedse ouswee,\n",
      "Him aitre bllesongth'sualit ate,\n",
      "It, thingeat strate,\n",
      "Of agite bllesingme cued.\n",
      "Wous, for whist reders oury i hapt he pial frome,\n",
      "\n",
      "Acaint beauty hespaive-ts but liked upharind from \n",
      "Uss ond gurous spalst\n",
      "A from and thees,\n",
      "Uschasils strate,\n",
      "Aitre blleslarted wherradst gotelrt list)\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 73.094059\n",
      "\n",
      "Hewslren they strare,\n",
      "And acoue ane the whemy eyou uso shapls;\n",
      "Athou mend whor bore that sweear is faep,\n",
      "Heab'st banter impordue sucest,\n",
      "Woul of thy strare,\n",
      "Ad art a mork epure yen that she fots khe mendour \n",
      "To formigong thy pert nothem makond am a may,\n",
      "A dour amoos glree weest by self thy link,\n",
      "Time so ullat sight,\n",
      "All least the whemy eyou uso shapls;\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 73.199625\n",
      "\n",
      "Pith the wiks soser mest\n",
      "In badite and whened by in cest.\n",
      "Lyess new full vateed samint i kect ence ait then \n",
      "Peace:\n",
      "Wort nine whink the wiclioul,\n",
      "Dabence and whened by in cest.\n",
      "Word every sood,\n",
      "A doth and veed.\n",
      "Urd'en hull were my ampew lald weed of in thel, ba\n",
      "All leart, fintimaw love word gents meath serose f\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 73.297512\n",
      "\n",
      "Mitsured rieth or thee.\n",
      "In callle anker found yeens as shile,\n",
      "It will prayehtl theaut my wild ditan and footaine\n",
      "Me allle anget in.\n",
      "Wotlenced whink the wideliy gances en(in dofiren;\n",
      "\n",
      "Da for dayse?\n",
      "Useciences thy silfou,\n",
      "A for dayse:\n",
      "Ureed,\n",
      "And?a coserecerne weets as shile,\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 72.088998\n",
      "\n",
      "Dither mand which pfinin it you fair cf ala mast o\n",
      "And afrom and the vee, and a wor liked:\n",
      "Atir blalj)s onb on not nomn!\n",
      "Deaced,\n",
      "Wroube.\n",
      "Ad cor barter devilds exracrow of thun here serfer\n",
      "To be, iethere pines,\n",
      "A drop and the vee, and a wor liked:\n",
      "To kinfetergn the subjint)\n",
      "And eacirer devilds exracrow of thun here serfer c\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 72.536167\n",
      "\n",
      "I wounce ponter, \n",
      "End \n",
      "Frowied in weref then sank, an hapt an me bo your \n",
      "I a doth anots ansieds blocerpe,\n",
      "With leng whink pindse\n",
      "Ad brigh hinh bets awanly as:\n",
      "Usieight whinj pindse\n",
      "A drie anous clove weets assedpele.\n",
      "Us menomy ors pleatuder wee heaved and blave,\n",
      "And daest thendel't ets aspeet?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = test.model(num_iterations=35000, n_a=50, seq_length=10)\n",
    "\n",
    "#(input_text_for_training, data, ix_to_char, char_to_ix, num_iterations, n_a , sample_lines, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a372e994d690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model to disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

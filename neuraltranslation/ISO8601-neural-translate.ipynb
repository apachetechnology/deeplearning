{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date to ISO8601 Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Translating human readable dates into machine readable dates\n",
    "\n",
    "The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) and translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Dataset\n",
    "\n",
    "The model is trained on a dataset of 100000 human readable dates and their equivalent, standardized, machine readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:07<00:00, 13361.73it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 100000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.09.70', '1970-09-10'),\n",
       " ('4/28/90', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('tuesday july 8 2008', '2008-07-08'),\n",
       " ('08 sep 1999', '1999-09-08'),\n",
       " ('1 jan 1981', '1981-01-01'),\n",
       " ('monday may 22 1995', '1995-05-22')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Neural machine translation with attention\n",
    "\n",
    "\n",
    "\n",
    "### 2.1 - Attention mechanism\n",
    "\n",
    "Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are some properties of the model: \n",
    "\n",
    "- There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\\langle t\\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\\langle t\\rangle}$ and the hidden cell state $c^{\\langle t\\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\\langle t-1 \\rangle}$ as input; it only takes $s^{\\langle t\\rangle}$ and $c^{\\langle t\\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. \n",
    "\n",
    "- We use $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. \n",
    "\n",
    "- The diagram on the right uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (100000, 30)\n",
      "Y.shape: (100000, 10)\n",
      "Xoh.shape: (100000, 30, 37)\n",
      "Yoh.shape: (100000, 10, 11)\n",
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Target after preprocessing (one-hot): [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)\n",
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    e = densor1(concat)\n",
    "    energies = densor2(e)\n",
    "    alphas = activator(energies)\n",
    "    context = dotor([alphas,a])\n",
    "       \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. \n",
    "    a = Bidirectional(LSTM(n_a,return_sequences=True), merge_mode='concat',input_shape=(m, Tx, n_a*2))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t \n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        s, _, c = post_activation_LSTM_cell(context,initial_state = [s, c])\n",
    "      \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM \n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list \n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. \n",
    "    model =  Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 30, 64)        17920       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 30, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 128)       0           bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[1][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[2][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[3][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[4][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[5][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[6][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[7][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[8][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 30, 10)        1290        concatenate_1[0][0]              \n",
      "                                                                   concatenate_1[1][0]              \n",
      "                                                                   concatenate_1[2][0]              \n",
      "                                                                   concatenate_1[3][0]              \n",
      "                                                                   concatenate_1[4][0]              \n",
      "                                                                   concatenate_1[5][0]              \n",
      "                                                                   concatenate_1[6][0]              \n",
      "                                                                   concatenate_1[7][0]              \n",
      "                                                                   concatenate_1[8][0]              \n",
      "                                                                   concatenate_1[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 30, 1)         11          dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "                                                                   dense_1[2][0]                    \n",
      "                                                                   dense_1[3][0]                    \n",
      "                                                                   dense_1[4][0]                    \n",
      "                                                                   dense_1[5][0]                    \n",
      "                                                                   dense_1[6][0]                    \n",
      "                                                                   dense_1[7][0]                    \n",
      "                                                                   dense_1[8][0]                    \n",
      "                                                                   dense_1[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_2[0][0]                    \n",
      "                                                                   dense_2[1][0]                    \n",
      "                                                                   dense_2[2][0]                    \n",
      "                                                                   dense_2[3][0]                    \n",
      "                                                                   dense_2[4][0]                    \n",
      "                                                                   dense_2[5][0]                    \n",
      "                                                                   dense_2[6][0]                    \n",
      "                                                                   dense_2[7][0]                    \n",
      "                                                                   dense_2[8][0]                    \n",
      "                                                                   dense_2[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 64)         0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 64), (None, 6 33024       dot_1[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_1[1][0]                      \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "                                                                   dot_1[2][0]                      \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[1][2]                     \n",
      "                                                                   dot_1[3][0]                      \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[2][2]                     \n",
      "                                                                   dot_1[4][0]                      \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[3][2]                     \n",
      "                                                                   dot_1[5][0]                      \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[4][2]                     \n",
      "                                                                   dot_1[6][0]                      \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[5][2]                     \n",
      "                                                                   dot_1[7][0]                      \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[6][2]                     \n",
      "                                                                   dot_1[8][0]                      \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[7][2]                     \n",
      "                                                                   dot_1[9][0]                      \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 11)            715         lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999,decay=0.01) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 348s - loss: 0.0199 - dense_3_loss_1: 2.6446e-04 - dense_3_loss_2: 1.5047e-04 - dense_3_loss_3: 3.3903e-04 - dense_3_loss_4: 7.4997e-04 - dense_3_loss_5: 6.2717e-06 - dense_3_loss_6: 0.0018 - dense_3_loss_7: 0.0020 - dense_3_loss_8: 7.3769e-06 - dense_3_loss_9: 0.0103 - dense_3_loss_10: 0.0043 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 1.0000 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9995 - dense_3_acc_7: 0.9999 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.9974 - dense_3_acc_10: 0.9991   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb987b0ac18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-05-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Visualizing Attention \n",
    "\n",
    "\n",
    "Example of someone's birthday \"Saturday 9 May 2018\" to \"2018-05-09\". If we visualize the computed $\\alpha^{\\langle t, t' \\rangle}$ we get this: \n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 8**: Full Attention Map</center></caption>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Getting the activations from the network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 30, 64)        17920       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 30, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 128)       0           bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[1][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[2][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[3][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[4][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[5][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[6][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[7][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[8][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 30, 10)        1290        concatenate_1[0][0]              \n",
      "                                                                   concatenate_1[1][0]              \n",
      "                                                                   concatenate_1[2][0]              \n",
      "                                                                   concatenate_1[3][0]              \n",
      "                                                                   concatenate_1[4][0]              \n",
      "                                                                   concatenate_1[5][0]              \n",
      "                                                                   concatenate_1[6][0]              \n",
      "                                                                   concatenate_1[7][0]              \n",
      "                                                                   concatenate_1[8][0]              \n",
      "                                                                   concatenate_1[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 30, 1)         11          dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "                                                                   dense_1[2][0]                    \n",
      "                                                                   dense_1[3][0]                    \n",
      "                                                                   dense_1[4][0]                    \n",
      "                                                                   dense_1[5][0]                    \n",
      "                                                                   dense_1[6][0]                    \n",
      "                                                                   dense_1[7][0]                    \n",
      "                                                                   dense_1[8][0]                    \n",
      "                                                                   dense_1[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_2[0][0]                    \n",
      "                                                                   dense_2[1][0]                    \n",
      "                                                                   dense_2[2][0]                    \n",
      "                                                                   dense_2[3][0]                    \n",
      "                                                                   dense_2[4][0]                    \n",
      "                                                                   dense_2[5][0]                    \n",
      "                                                                   dense_2[6][0]                    \n",
      "                                                                   dense_2[7][0]                    \n",
      "                                                                   dense_2[8][0]                    \n",
      "                                                                   dense_2[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 64)         0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 64), (None, 6 33024       dot_1[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_1[1][0]                      \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "                                                                   dot_1[2][0]                      \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[1][2]                     \n",
      "                                                                   dot_1[3][0]                      \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[2][2]                     \n",
      "                                                                   dot_1[4][0]                      \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[3][2]                     \n",
      "                                                                   dot_1[5][0]                      \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[4][2]                     \n",
      "                                                                   dot_1[6][0]                      \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[5][2]                     \n",
      "                                                                   dot_1[7][0]                      \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[6][2]                     \n",
      "                                                                   dot_1[8][0]                      \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[7][2]                     \n",
      "                                                                   dot_1[9][0]                      \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 11)            715         lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb99a5744e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGsCAYAAAD9ro91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncAZrSZIa7v8B6MEUY3NzULwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb987b1fda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 May 1978\", num = 7, n_s = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

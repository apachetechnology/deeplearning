{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tensorflow for tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tokenize the words and sentences, building up a dictionary of all the words to make a corpus with tensorflow? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that\n",
    "- Punctuations are removed. \n",
    "- i and I get the same coding\n",
    "- ordered based on the count\n",
    "\n",
    "The next step will be to turn your sentences into lists of values based on these tokens. \n",
    "\n",
    "\n",
    "`texts_to_sequences` can encode sentences.  It can take any set of sentences, so it can encode them based on the word set that it learned from the one that was passed into fit on texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer =Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_idx=tokenizer.word_index\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(word_idx)\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data=[\n",
    "    'i realy love my dog',\n",
    "    'my dog loves my manatee'\n",
    "    ]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because 'loves' and 'manatee' are not trained, my dog my 1,3,1 are the encoding of the second sentence. \n",
    "\n",
    "### Out of vocab\n",
    "First of all, we really need a lot of training data to get a broad vocabulary or we could end up with sentences like, my dog my. Secondly, in many cases, it's a good idea to instead of just ignoring unseen words, to put a special value in when an unseen word is encountered. It can be done by add a property oov token to the tokenizer constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer =Tokenizer(num_words=100, oov_token='<oov>') #out of vocab\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_idx=tokenizer.word_index\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(word_idx)\n",
    "print(sequences)\n",
    "\n",
    "test_data=[\n",
    "    'i realy love my dog',\n",
    "    'my dog loves my manatee'\n",
    "    ]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "use `pad_sequences` from kears.preprocessing.sequence\n",
    "\n",
    "`pad_sequences(sequences, padding='post', maxlen=5`\n",
    "\n",
    "padds '0' from the front unless `padding='post'` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(sequences)\n",
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maxlen` sets the limit of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  3  2  4  0]\n",
      " [ 5  3  2  7  0]\n",
      " [ 6  3  2  4  0]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=5)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the longest sentence was truncated from the front. By passing `truncating='post'`, will truncate from the back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  3  2  4  0  0]\n",
      " [ 5  3  2  7  0  0]\n",
      " [ 6  3  2  4  0  0]\n",
      " [ 8  6  9  2  4 10]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, \n",
    "                       padding='post', \n",
    "                        maxlen=6,\n",
    "                        truncating='post')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, \n",
    "                       padding='post', \n",
    "                        maxlen=10,\n",
    "                        truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 1 3 2 4 0 0 0 0 0]\n",
      " [2 4 1 2 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "print(word_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on larger text data\n",
    "Sample text data\n",
    "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-05 20:37:18--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.13.240, 2607:f8b0:4004:810::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.13.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5643545 (5.4M) [application/json]\n",
      "Saving to: ‘/tmp/sarcasm.json’\n",
      "\n",
      "/tmp/sarcasm.json   100%[===================>]   5.38M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2019-08-05 20:37:18 (86.5 MB/s) - ‘/tmp/sarcasm.json’ saved [5643545/5643545]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
    "    -O /tmp/sarcasm.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of JSON file\n",
    "`[\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5\", \"headline\": \"former versace store clerk sues over secret 'black code' for minority shoppers\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365\", \"headline\": \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697\", \"headline\": \"mom starting to fear son's web series closest thing she will have to grandchild\", \"is_sarcastic\": 1},\n",
    "{\"article_link\": \"https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302\", \"headline\": \"boehner just wants wife to listen, not come up with alternative debt-reduction ideas\", \"is_sarcastic\": 1},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb\", \"headline\": \"j.k. rowling wishes snape happy birthday in the most magical way\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/advancing-the-worlds-women_b_6810038.html\", \"headline\": \"advancing the world's women\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/how-meat-is-grown-in-a-lab_us_561d1189e4b0c5a1ce607e86\", \"headline\": \"the fascinating case for eating lab-grown meat\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/boxed-college-tuition-ben_n_7445644.html\", \"headline\": \"this ceo will send your kids to school, if you work for his company\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://politics.theonion.com/top-snake-handler-leaves-sinking-huckabee-campaign-1819578231\", \"headline\": \"top snake handler leaves sinking huckabee campaign\", \"is_sarcastic\": 1},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/fridays-morning-email-inside-trumps-presser-for-the-ages_us_58a6e33ee4b07602ad53a315\", \"headline\": \"friday's morning email: inside trump's presser for the ages\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/airline-passengers-tackle-man-who-rushes-cockpit-in-bomb-threat_us_59302e57e4b07572bdbf9460\", \"headline\": \"airline passengers tackle man who rushes cockpit in bomb threat\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://www.huffingtonpost.com/entry/facebook-healthcare_n_5926140.html\", \"headline\": \"facebook reportedly working on healthcare features and apps\", \"is_sarcastic\": 0},\n",
    "{\"article_link\": \"https://www.huffingtonpost.comhttp://www.theguardian.com/world/2016/may/31/north-korea-praises-trump-and-urges-us-voters-to-reject-dull-hillary\", \"headline\": \"north korea praises trump and urges us voters to reject 'dull hillary'\", \"is_sarcastic\": 0},`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import json\n",
    "\n",
    "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "print(word_index)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
